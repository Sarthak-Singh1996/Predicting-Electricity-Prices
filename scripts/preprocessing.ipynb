{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbd31640",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from sklearn.compose import ColumnTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9c59fad",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m      2\u001b[0m url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://raw.githubusercontent.com/Tobias-Neubert94/adam_monk_II/master/adam_monk_II/data/Price_Data_Updated.gzip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/adam_monk_II/lib/python3.10/site-packages/pandas/io/parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[0;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[1;32m    428\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[1;32m    430\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    491\u001b[0m \u001b[38;5;124;03m    DataFrame\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 493\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[1;32m    496\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    499\u001b[0m         )\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.6/envs/adam_monk_II/lib/python3.10/site-packages/pandas/io/parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[0;34m(engine)\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[0;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[1;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     68\u001b[0m     )\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[0;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "url = \"https://raw.githubusercontent.com/Tobias-Neubert94/adam_monk_II/master/adam_monk_II/data/Price_Data_Updated.gzip\"\n",
    "df = pd.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcb4228",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns to snake case and cleaner names\n",
    "new_names = [\n",
    "    \"date\",\n",
    "    \"temperature_berlin\", \"temperature_cologne\", \"temperature_frankfurt\", \"temperature_hamburg\", \"temperature_munich\",\n",
    "    \"prep_berlin\", \"prep_cologne\", \"prep_frankfurt\", \"prep_hamburg\", \"prep_munich\",\n",
    "    \"snow_berlin\", \"snow_cologne\", \"snow_frankfurt\", \"snow_hamburg\", \"snow_munich\",\n",
    "    \"windspeed_berlin\", \"windspeed_cologne\", \"windspeed_frankfurt\", \"windspeed_hamburg\", \"windspeed_munich\",\n",
    "    \"irradiation_berlin\", \"irradiation_cologne\", \"irradiation_frankfurt\", \"irradiation_hamburg\", \"irradiation_munich\",\n",
    "    \"future_price\",\n",
    "    \"gen_biomass\",\n",
    "    \"gen_ff_browncoallignite\",\n",
    "    \"gen_ff_coalderivedgas\",\n",
    "    \"gen_fossilgas\",\n",
    "    \"gen_fossilhardcoal\",\n",
    "    \"gen_fossiloil\",\n",
    "    \"gen_geothermal\",\n",
    "    \"gen_hydropumpedstorage\",\n",
    "    \"gen_hydrorunofriver\",\n",
    "    \"gen_hydrowaterreservoir\",\n",
    "    \"gen_nuclear\",\n",
    "    \"gen_other\",\n",
    "    \"gen_otherrenewable\",\n",
    "    \"gen_solar\",\n",
    "    \"gen_waste\",\n",
    "    \"gen_windoffshore\",\n",
    "    \"gen_windonshore\",\n",
    "]\n",
    "df.columns = new_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4087acc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set target at end of dataframe\n",
    "df = df[[c for c in df.columns if c != \"future_price\"] + [\"future_price\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306415d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter dataset for years 2015+ as generation data is not available as far back as 2003\n",
    "df = df[df.date.dt.year >= 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47897689",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aa6b225",
   "metadata": {},
   "source": [
    "## Addressing null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e4459c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"These are the columns with null values:\")\n",
    "df.loc[:, df.isnull().sum() > 0].isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de4fde6",
   "metadata": {},
   "source": [
    "We notice there are null values reported for Berlin and Munich across multiple weather measures (snow & irradiation for Berlin), (windspeed & snow for Munich).\n",
    "Let's check how many dates report NULL for both technologies at the same time to consider whether some rows should be dropped. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "758787b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    (df.snow_berlin.isnull())\n",
    "    & (df.irradiation_berlin.isnull())\n",
    "][\n",
    "    [\"date\"] + [col for col in df.columns if \"berlin\" in col]\n",
    "]\n",
    "# only six rows for berlin - fine to impute values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f9a9f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    (df.windspeed_munich.isnull())\n",
    "    & (df.snow_munich.isnull())\n",
    "][\n",
    "    [\"date\"] + [col for col in df.columns if \"munich\" in col]\n",
    "]\n",
    "# only one row for munich - even better"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0f3252",
   "metadata": {},
   "source": [
    "### Snow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff9fc6b",
   "metadata": {},
   "source": [
    "Values are most frequently missing for snowfall. Let's assume snowfall is relevant mostly for winter months (10-2), and research when snowfall values are missing. Ideally they should be in the spring/summer months (3-9), with an even spread across years."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4222dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\n",
    "    (df.snow_munich.isnull())\n",
    "    | (df.snow_berlin.isnull())\n",
    "    | (df.snow_cologne.isnull())\n",
    "][[\"date\", \"snow_munich\", \"snow_berlin\", \"snow_cologne\"]].groupby([df.date.dt.year, df.date.dt.month]).count()\n",
    "\n",
    "# This looks good - across years, evenly missing mostly between months 5-9."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b294ba8",
   "metadata": {},
   "source": [
    "About 80% of the null values for snowfall are reported in summer months.\n",
    "Let's take a simple approach for snow and assume NULL values can be filled with 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478f357f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df.columns:\n",
    "    if \"snow\" in col:\n",
    "        df[col] = df[col].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba68f983",
   "metadata": {},
   "source": [
    "### Solar irradiation in Berlin & wind speed in Munich"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77d9a47d",
   "metadata": {},
   "source": [
    "All of the values for irradiation in Berlin are missing in 2022.\n",
    "To backfill, we could consider replacing with the values from the previous year, but we might not get an exact date because of the fact that we don't have pricing data for the weekend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24897874",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[(df.irradiation_berlin.isnull())][[\"date\", \"irradiation_berlin\"]].groupby([df.date.dt.year, df.date.dt.month]).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d584cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted(df: pd.DataFrame, date_col: str, fill_col: str, period:int,) -> pd.DataFrame:\n",
    "    df[\"shifted\"] = df.groupby([df[date_col].dt.month, df[date_col].dt.day])[fill_col].shift(period)\n",
    "    df[fill_col] = np.where(\n",
    "        df[fill_col].isnull(),\n",
    "        df[\"shifted\"],\n",
    "        df[fill_col]\n",
    "    )\n",
    "    df.drop(columns=\"shifted\", inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a72e030",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shifted(df, \"date\", \"irradiation_berlin\", 1)\n",
    "df = shifted(df, \"date\", \"windspeed_munich\", -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c65a682f",
   "metadata": {},
   "source": [
    "## Correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "963f85f6",
   "metadata": {},
   "source": [
    "Here, I start by splitting our data into features and target to start working more on the features dataset. Note I take out the date column but add it back in later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526b73f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine features and target\n",
    "X = df.drop(columns=[\"date\", \"future_price\"])\n",
    "features = list(X.columns)\n",
    "y = df[\"future_price\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b930ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First let's look at the correlation of all the features\n",
    "sns.heatmap(X.corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8e3f59",
   "metadata": {},
   "source": [
    "Takeaways: there are two distinct splits of data between weather metrics and power generation.\n",
    "\n",
    "1. Weather features for given cities are highly correlated with eachother, such as temperature and irradiaton, but precipitation and snowfall are less so. Temperature and irraditiation are also correlated with eachother for a given city, which makes sense (if it's sunny, the temperature is higher, and vice-versa).\n",
    "2. There are some high correlations among generation technologies too.\n",
    "2. Let's split our features into two: weather and generation, remove some colinearity in those two cuts, and then consider the dataframe as a whole."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4849c3c",
   "metadata": {},
   "source": [
    "### Weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "896c7314",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(pd.DataFrame(X).iloc[:, 0: 25].corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5b238db",
   "metadata": {},
   "source": [
    "Conclusion: let's make a single feature for temperature, irradiation, and windspeed, and look at the matrix again. It will be the mean across the cities we have selected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2702708",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_weather(df: pd.DataFrame, measures: List[str]) -> pd.DataFrame:\n",
    "    for measure in measures:\n",
    "        col = f\"{measure}_germany\"\n",
    "        df[col] = df[\n",
    "            [c for c in df.columns if measure in c]\n",
    "        ].mean(axis=1)\n",
    "        df.drop(columns=[\n",
    "            c for c in df.columns if measure in c and \"germany\" not in c\n",
    "        ], inplace=True)\n",
    "        df.insert(0, col, df.pop(col))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69bd417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = average_weather(X, [\"temperature\", \"irradiation\", \"windspeed\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ee466f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.iloc[:, :13].corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17b61124",
   "metadata": {},
   "source": [
    "### Generation technologies correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8b0ccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(X.iloc[:, 13:].corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02885b43",
   "metadata": {},
   "source": [
    "Initially I thought about grouping some of these technologies (hydro, coal, gas, wind, etc.), but given the correlations between technologies aren't so obvious, I opted to leave them as is and let the PCA analysis make the decision."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e34ff2e",
   "metadata": {},
   "source": [
    "## Scaling & PCA analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c7d8c45",
   "metadata": {},
   "source": [
    "In order to run a PCA analysis, the data needs to be centered around the mean, so we use the StandardScaler() to scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839210e4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_features = list(X.columns)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)\n",
    "X = pd.DataFrame(scaler.transform(X), columns=X_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106cfd01",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(X)\n",
    "\n",
    "# Access our 30 PCs \n",
    "W = pca.components_\n",
    "\n",
    "# Print PCs as COLUMNS\n",
    "W = pd.DataFrame(W.T,index=X_features, columns=[f'PC{i}' for i in range(1, 31)])\n",
    "W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d372890",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.heatmap(W.corr(), cmap='coolwarm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71ed3875",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c298185f",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(pca.explained_variance_ratio_)\n",
    "plt.xlabel('Principal Component'); plt.ylabel('% explained variance');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a291d",
   "metadata": {},
   "source": [
    "Almost 50% of the variance in the dataset can be explained in the first three axes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d094030c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.ylim(ymin=0)\n",
    "plt.title('cumulated share of explained variance')\n",
    "plt.xlabel('# of principal component used');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c2140f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=25).fit(X) # Fit a PCA with  15 components\n",
    "X_25 = pd.DataFrame(pca.fit_transform(X), columns=[\"PC\" + str(i) for i in range(25)\n",
    "]) # Project data into 25 dimensions\n",
    "X_25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ab654a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test PCA data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_25, y, test_size=0.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687f9c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compared to no PCA analysis with all features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab55a2c3",
   "metadata": {},
   "source": [
    "Conclusion: I think let's ignore the option to reduce features with PCA analysis, and just use it to remove correlation between features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68e32200",
   "metadata": {},
   "source": [
    "# Build pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c70028bb",
   "metadata": {},
   "source": [
    "To build pipeline:\n",
    "\n",
    "1. fill null values for snow columns with 0 using SimpleImputer.\n",
    "2. apply shifted() method to two other columns with null values using FunctionTransformer.\n",
    "3. Average temperature, windspeed and irradiation with average_weather() method, using FunctionTransformer.\n",
    "4. Scale all features for PCA analysis with StandardScaler.\n",
    "5. Build in PCA analysis.\n",
    "6. Add back in date column?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a09399d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "import pandas as pd \n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f8346f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a63418",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec78f9aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94173e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(columns='future_price')\n",
    "y = df['future_price']\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)\n",
    "X_train.shape, X_test.shape, y_train.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eef3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"median\")),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "pipeline.fit(X_train[['date']])\n",
    "pipeline.transform(X_train[['date']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4622ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8174f43f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_transformer = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy=\"mean\")),\n",
    "    ('standard_scaler', StandardScaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb92132",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import set_config; set_config(display='diagram')\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19de4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply shifted() method to two other columns with null values using FunctionTransformer.\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import FunctionTransformer \n",
    "def shifted_method(prep_berlin, shift_value=0):\n",
    "    shifted_prep_berlin=shift_prep_berlin, shift_value=0\n",
    "    return shifted_prep_berlin \n",
    "shift_value=0\n",
    "\n",
    "#transformer=FunctionTransformer (func=shifted_method, kw=args={'shift_value'})    \n",
    "    \n",
    "#prep_berlin_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)\n",
    "#prep_cologne_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dac7966",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Average temperature, windspeed and irradiation with average_weather() method, using FunctionTransformer.\n",
    "\n",
    "#  X = average_weather(X, [\"temperature\", \"irradiation\", \"windspeed\"])\n",
    "#  with function transformer \n",
    "\n",
    "#calculate the mean (=average of all the data) of each weather parameter \n",
    "\n",
    "def mean1(temperature_berlin):\n",
    "    return(float(sum(temperature_berlin)) / len(temperature_berlin))\n",
    "def mean(temperature_cologne):\n",
    "    return(float(sum(temperature_cologne)) / len(temperature_cologne))\n",
    "def mean3(temperature_frankfurt):\n",
    "    return(float(sum(temperature_frankfurt)) / len(temperature_frankfurt))\n",
    "def mean4(temperature_hamburg):\n",
    "    return(float(sum(temperature_hamburg)) / len(temperature_hamburg))\n",
    "def mean5(temperature_munich):\n",
    "    return(float(sum(temperature_munich)) / len(temperature_munich))\n",
    "def mean(temperature_temperature):\n",
    "    return(float(sum(mean1+mean2+mean3+mean4+mean5)) / len(mean1+mean2+mean3+mean4+mean5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ffdcefa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#another method \n",
    "\n",
    "import pandas as pd\n",
    "df.groupby(['temperature_berlin']).mean()\n",
    "\n",
    "import pandas as pd\n",
    "df.groupby(['temperature_cologne']).mean()\n",
    "\n",
    "import pandas as pd\n",
    "df.groupby(['temperature_frankfurt']).mean()\n",
    "\n",
    "import pandas as pd\n",
    "df.groupby(['temperature_hamburg']).mean()\n",
    "\n",
    "import pandas as pd\n",
    "df.groupby(['temperature_munich']).mean()\n",
    "\n",
    "# create a new column with the average of the averages Temperature of the 5 cities \n",
    "\n",
    "df = pd.DataFrame({'average_temperature':[temperature_berlin, temperature_cologne, temperature_frankfurt, \n",
    "                                          temperature_hamburg, temperature_munich]})\n",
    "df['average_temperature'] = df['temperature_berlin'] + df['Cost'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9074078b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac2a65b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale all features for PCA analysis with StandardScaler\n",
    "scaler = StandardScaler()\n",
    "df=df.set_index('date')\n",
    "scaler.fit(df)\n",
    "scaled_df = scaler.transform(df)\n",
    "pca = PCA()\n",
    "principal_components = pca.fit_transform(scaled_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508072cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Build in PCA analysis\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca.fit(df)\n",
    "transformed_data = pca.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933d4e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#fit pipeline to the training set \n",
    "pipeline.fit(X_train)\n",
    "#tranform both the training set and the test set \n",
    "X_train_transformed = pd.DataFrame(pipeline.transform(X_train))\n",
    "X_train_transformed\n",
    "X_test_transformed = pd.DataFrame(pipeline.transform(X_test))\n",
    "X_test_transformed\n",
    "#standardize X-test and x_train \n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_scaled = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0867387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "snow_imputer = SimpleImputer(strategy=\"constant\", fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14db8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"snow_transformer\", snow_imputer, [col for col in df.columns if \"snow\" in col]),\n",
    "],\n",
    "    remainder=\"passthrough\",)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9ab1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fb7cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer([\n",
    "    (\"prep_berlin_transformer\", prep_berlin_imputer, [col for col in df.columns if \"prep_berlin\" in col]),\n",
    "], remainder=\"passthrough\",)\n",
    "    (\"prep_cologne_transformer\", prep_cologne_imputer, [col for col in df.columns if \"prep_cologne\" in col]),\n",
    "], remainder=\"passthrough\",)\n",
    "preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564ad87b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9222096",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2adb76da",
   "metadata": {},
   "source": [
    "# Data visualisation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f5d85c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4566648e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_new = df[['date', 'future_price']]\n",
    "print(df_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5bc5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(df['date'], df['future_price'])\n",
    "plt.xlabel('date')\n",
    "plt.ylabel('future_price')\n",
    "plt.title('Evolution of prices')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec3d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple graph of evolution of future prices \n",
    "plt.plot(date_x, future_price_y)\n",
    "plt.ylabel('Future electricity prices')\n",
    "plt.title('Evolution of future electricity pricesin Germany')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57633a1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68866076",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9db878a5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73bd0f32",
   "metadata": {},
   "source": [
    "# Appendix: Charting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f675d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_weather_patterns(city: str):\n",
    "\n",
    "    temp = [col for col in df.columns if city in col and \"temperature\" in col]\n",
    "    prep = [col for col in df.columns if city in col and \"prep\" in col]\n",
    "    wind = [col for col in df.columns if city in col and \"windspeed\" in col]\n",
    "    snow = [col for col in df.columns if city in col and \"snow\" in col]\n",
    "    irr = [col for col in df.columns if city in col and \"irradiation\" in col]\n",
    "\n",
    "    # Start figure\n",
    "    plt.figure(figsize=(10,15))\n",
    "\n",
    "    # Temperature\n",
    "    plt.subplot(5, 1, 1)\n",
    "    plt.plot(df.date, df[temp], c=\"black\", linewidth=0.5)\n",
    "    plt.title(\"Average temperature\")\n",
    "    # Precipitation\n",
    "    plt.subplot(5, 1, 2)\n",
    "    plt.plot(df.date, df[prep], c='black', linewidth=0.5)\n",
    "    plt.title(\"Precipitation\")\n",
    "    # Wind speed\n",
    "    plt.subplot(5, 1, 3)\n",
    "    plt.plot(df.date, df[wind], c='black', linewidth=0.5)\n",
    "    plt.title(\"Wind speed\")\n",
    "    # Snow fall\n",
    "    plt.subplot(5, 1, 4)\n",
    "    plt.plot(df.date, df[snow], c='black', linewidth=0.5)\n",
    "    plt.title(\"Snow\")\n",
    "    # Solar irradiation\n",
    "    plt.subplot(5, 1, 5)\n",
    "    plt.plot(df.date, df[irr], c='black', linewidth=0.5)\n",
    "    plt.title(\"Solar irradiation\")\n",
    "    \n",
    "    plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e017dd31",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_weather_patterns(\"munich\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1509dbdc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
